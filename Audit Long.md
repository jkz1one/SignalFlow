Pipeline Structure and Workflow Alignment

Core Pipeline Structure

The Stock Screener 3.7 backend has two main orchestrator scripts:
	•	daily_refresh.py – Handles all data scraping tasks in sequence, then triggers cache maintenance. It runs each morning (recommended ~9:40 AM ET, just after market open) to gather fresh data ￼. The tasks executed (in order) are:
	•	Build Universe → outputs universe_cache.json
	•	Scrape TradingView Signals → tv_signals.json
	•	Scrape Sector ETF prices → sector_etf_prices.json
	•	Scrape 5-minute opening range (9:30–9:40) candles → candles_5m.json
	•	Fetch Multi-day High/Low levels → multi_day_levels.json
	•	Fetch Short Interest data → short_interest.json
After all scrapers run, daily_refresh.py performs a one-time cache cleanup and audit routine ￼. This cleanup uses the last valid NYSE trading day to decide what’s stale ￼. It then prints a cache audit report for data health (missing tickers, etc.). The audit logic is indeed invoked only once at the end of the refresh (not after each scraper), as expected ￼. This matches the summary: the scrapers populate the cache, and then obsolete files (from previous days) are removed and an audit is performed.
	•	run_pipeline.py – Orchestrates enrichment, scoring, and watchlist output generation ￼. This is run after the daily refresh to build the scored universe and final watchlist. Its steps are:
	1.	Enrichment – calls backend/enrich_universe.py to combine all cached data into an enriched universe JSON (including adding signals like price/volume changes, sector info, etc.) ￼.
	2.	Cache Freshness Check – verifies that all expected cache files from the refresh are present and up-to-date ￼ ￼, aborting if any are missing. (This uses the same NYSE calendar logic to ensure data is from the latest market day.)
	3.	Cache Cleanup – runs cache_manager.py which performs the same cleanup & audit as the refresh (to ensure nothing stale remains before scoring) ￼.
	4.	Scoring – invokes the screening module to score each ticker’s signals (Tier 1/2/3 logic and risk tagging) ￼. This produces a universe_scored_{date}.json in the cache.
	5.	Watchlist Build – generates the final AutoWatchlist JSON by filtering the scored universe (applying tier and risk filters) ￼.
The pipeline then prints completion and the system is ready for the frontend/API to serve the results. This flow aligns with the audit description: enrichment → scoring → watchlist, with smart cache checks and a post-run cleanup/audit ￼.

Confirmation: The above structure matches the audit summary. daily_refresh.py is solely responsible for scraping and caching raw data, while run_pipeline.py handles enriching that data, scoring tickers, and producing outputs. The division of labor is exactly as described in the documentation ￼ ￼.

Cache Cleanup & Audit Logic

The cache management is implemented as summarized, with correct usage of NYSE trading days for date logic. The code uses the pandas_market_calendars library to get the last NYSE market date and treats files from that date as “today’s” files ￼ ￼. This ensures that on Mondays (for example) the “last market day” is the previous Friday, so the cleanup won’t delete Friday’s files if the refresh hasn’t run yet on Monday.

Both cleanup functions and the audit are invoked together once per run of the refresh or pipeline. In daily_refresh.py, they are called after all scraping tasks finish ￼. In run_pipeline.py, the call to cache_manager.py internally runs cleanup + audit once as well ￼. There is no redundant multiple invocation; each run of the process triggers one cleanup sweep and one audit report, as expected.

The audit output checks each expected cache file for freshness and completeness. For example, it warns if TradingView data is missing timestamps, if any sector ETFs are missing, if the 5m candle file has empty entries, etc. ￼ ￼ ￼. It also specifically references the count of short-interest tickers and expects a reasonable number (warning if, say, fewer than 50 tickers have data) ￼. This logic matches the intended “health diagnostics” mentioned in the summary. All of these audit checks use the last market day reference as described. The summary’s note that cache cleanup uses “last valid market day” is confirmed in code and in documentation ￼.

Confirmation: The cache cleanup and audit mechanisms are correctly implemented and only run once per refresh/pipeline run. They do indeed reference NYSE market calendar data for determining staleness, in line with the audit summary ￼.

Enrichment Timing Dependencies

The audit highlighted that certain data should only be fetched after the market opens to be accurate (e.g. TradingView signals after 9:30 AM, and the first 5-minute candle after ~9:35-9:40 AM). The current implementation relies on schedule/documentation rather than code-based enforcement for these timing dependencies:
	•	The README/Audit docs explicitly instruct that daily_refresh.py be run around 9:40 AM ET (just after the first two 5-minute candles of the session) ￼. This guidance is to ensure that the 5m candle scraper captures the 9:30–9:35 and 9:35–9:40 candles and that TradingView “premarket” signals have updated to regular session values. The code itself does not check the current time before running a task – it assumes the operator runs it at the appropriate time. If run earlier, the scraper_candles_5m.py might simply produce empty or partial data (the audit will flag “tickers have no 5m candles” if so ￼).
	•	The tasks list in daily_refresh.py is ordered such that TradingView signals are scraped before the 5m candle data ￼. This is logical because TradingView provides real-time price/volume and can be polled immediately at 9:30 for initial values, whereas the 5m candle task waits until after 9:35. In practice, by the time the script gets to the candle scraper (fourth in the list), it will likely be ~9:40 AM, so the timing works out.

The enrichment process (enrich_universe.py) consumes whatever data is in the cache. It assumes that tv_signals.json includes live price/volume as of open and that candles_5m.json has the opening range. If daily_refresh.py was run at the recommended time, these assumptions hold. This is clearly documented in the usage instructions (run after 9:30, optimal ~9:40) ￼, which covers the dependency. There is no hardcoded delay in the code, but the design and docs enforce the timing.

Confirmation: The timing dependencies are documented and handled via the run schedule. The audit summary’s point about scraping TV signals after 9:30 and 5m candles after 9:40 is addressed by instructing the user to run the refresh at 9:40 AM ￼. The code’s task ordering aligns with this, and the audit tool will catch missing data if run too early, which is effectively a safeguard.

Removal of Redundant/Unused Scripts

All previously noted redundant or old scripts have been cleaned up in the current version. Notably, there is no separate yf_enrichment.py script anymore – the functionality of Yahoo Finance data enrichment has been merged into the TradingView scraper and the enrichment process. In fact, the project notes explicitly mention “TradingView signal scraper merged with yfinance” and “Removed deprecated yfinance_updated field” ￼. This indicates that any old Yahoo Finance enrichment step has been consolidated, and indeed we see that the TradingView data collected includes some Yahoo info (it adds a yfinance_updated timestamp internally, which is then dropped during enrichment) rather than requiring a standalone script.

Similarly, any “old universe builder” scripts have been replaced by the current universe_builder.py. The active universe construction now happens in signals/universe_builder.py (invoked by the refresh) which uses anchor lists and index constituents. No other legacy universe-building scripts are referenced in the pipeline. We verified that the signals/ directory only contains the expected modules (the ones invoked by daily_refresh.py) and nothing extra ￼. The audit summary’s concern about old or unused code is resolved – the repository is organized and free of dead code in the workflow. Each script in signals/ corresponds to a needed step, and all are invoked.

For example, yf_enrichment.py is not present, and older variants of universe building (if any existed previously) are not in use. The Universe Builder is up-to-date: it pulls S&P 500 and Nasdaq-100 constituents and uses a static ticker list for anchors, exactly as described. (It is still a version 1 implementation – see next section for improvements planned – but there is no second older script hanging around; the current one is the source of truth.)

Confirmation: There are no unused or redundant scripts in the current workflow. The audit summary’s mention of things like yf_enrichment.py or deprecated builders being inactive is accurate – those have been removed or merged into the new pipeline. The codebase has been streamlined so that only the current scrapers and pipeline pieces are in play ￼.

Timing/Enrichment Enforcement vs. Documentation

(Note: This point is essentially addressed above under Timing Dependencies. We can combine it or skip if redundant. It appears the main concern was already discussed, so we ensure it’s covered and move on.)

Outstanding Improvement Opportunities

The audit had identified some areas for improvement. We confirm that these remain outstanding in the current version (i.e. they have not been fully implemented yet, so the audit suggestions are still relevant):
	•	Combining Scrapers with Enrichment: The process is still split into separate steps and even separate processes. Each scraper runs in its own subprocess during daily_refresh.py ￼, and then enrich_universe.py reads their outputs from disk. There is an opportunity to streamline this – for example, by converting scraper scripts into callable functions and possibly integrating the refresh and enrich steps to avoid I/O overhead. In fact, the project roadmap explicitly notes plans to “convert all scrapers to callable functions (vs raw scripts)” and to possibly add a unified runner ￼. As of now, those changes have not been made – the architecture still reflects the original design of discrete scraping followed by enrichment. So the suggestion to combine or better integrate these steps (for efficiency or simplicity) is still valid.
	•	Frontend Risk Toggle Issue: The audit mentioned a bug/issue with the frontend “risk: off” toggle (the UI control that should include/exclude risk-flagged tickers). This issue is still present in the current code; there hasn’t been a commit indicating a fix, and the project TODOs list it as an open item. The project’s goals section lists “Fix frontend risk toggle logic for hiding risk-blocked tickers” as a task in progress ￼. Since the backend’s output clearly marks low_liquidity/wide_spread risk flags, this is purely a frontend logic problem. No changes on the backend have addressed it yet, so the audit’s note that this needs fixing remains current.
	•	Universe Builder v2: An improved universe builder (allowing dynamic or modifiable anchor sources) was noted as a future improvement. The current universe_builder.py still uses hard-coded anchor tickers and static index lists ￼ ￼. It does fetch fresh index constituents (for S&P 500 and Nasdaq 100), but the “level” assignments (L0, L1, L2 tiers for sources) are based on fixed logic. The audit’s suggestion to make the universe construction more flexible is still relevant. This is also mentioned in the roadmap (Universe Builder v2) ￼ and has not been implemented yet.
	•	Additional Logging and Configurability: The summary’s recommendations like adding more logging to the pipeline, exposing threshold parameters (e.g., relative volume threshold) for easier tuning, etc., are improvements yet to be done. The code currently uses simple print statements for progress, and there is no user-configurable settings file for things like volume thresholds – these remain hardcoded (e.g., the high_volume flag uses 1,000,000 shares, relative volume >1.5 for certain signals, etc., all set in code ￼). The audit’s points on these fronts are still valid going forward.

In short, the “Next Steps” items from the audit report (and project README) are on point: they are reflected in the project’s to-do list and have not been addressed yet ￼. The audit document can retain those as recommendations since they remain improvement areas.

New Features/Changes Since Last Audit

Since the last audit summary was written, a few notable updates have been made to the repository. The audit documentation should be updated to reflect these:
	•	5-Minute Opening Range Data: A new 5m candle scraper was added to the daily refresh flow. This was likely not in the previous version of the code. It fetches the high/low prices in the 9:30–9:40 AM window for each ticker. In code, this outputs candles_5m.json and the enrichment step uses it to determine opening range breakout signals (flags like break_above_range, break_below_range, and adds the fields range_930_940_high/low) ￼ ￼. The README’s Completed Features now include “candles” as part of enrichment ￼. The audit summary should explicitly mention this scraper and the signals it enables, if not already noted.
	•	Integrated Short Interest Data: The pipeline now incorporates short interest: fetch_short_interest.py loads short float percentage data (from FINRA/Nasdaq sources). During enrichment, tickers with high short interest (≥18% float), high relative volume, and a significant price move get a squeeze_watch flag ￼ ￼. This is a new automated tag. This feature appears to be new or improved; the summary should confirm the presence of the short interest step (it was listed in the tasks, but now we know how it’s used in scoring). The audit report can note that short interest is now fully integrated into the signals ￼.
	•	Enriched Timestamp: Each entry in the enriched universe now gets an enriched_timestamp field with the ISO timestamp of when the enrichment ran ￼. This was added to help with data freshness tracking. The summary may not have mentioned it before; it’s a minor addition but worth noting for completeness (the documentation did list it as completed ￼).
	•	Cache Audit Enhancements: The cache audit now does granular checks (as described above). If the previous audit summary only generally mentioned an audit, we can now specify that it checks for missing data points (timestamps, missing tickers per category, etc.). This is an improvement that ensures data quality each run ￼ ￼ ￼.
	•	Documentation Alignment: The README and project docs have been updated to reflect many of these changes (e.g. the “Enrichment: TV price/vol, candles, short interest, multi-day highs” is marked complete ￼). One discrepancy to note: the README’s Daily Refresh section still lists “Build enriched output → universe_enriched_*.json” under the refresh steps ￼. In practice (as described), the enriched output is generated when run_pipeline.py calls the enrichment script, not directly by daily_refresh.py. The documentation might be treating the entire process (refresh + enrich) as one morning routine, but for accuracy the audit summary can clarify that the enriched file is produced during the pipeline stage, after the scrapers. This is a minor doc tweak to consider.

Suggested Edits to Audit Summary:
	•	Under Daily Refresh, list the “Scrape 5m Candles → candles_5m.json” task explicitly (if it was missing). This is now a key part of the refresh data collection.
	•	Clarify that “Build enriched output” is the first step of the pipeline (triggered by run_pipeline.py calling enrich_universe), rather than something daily_refresh.py itself does. The refresh prepares all inputs, and the pipeline builds the enriched universe.
	•	Retain the notes about cache cleanup and audit, but you can now say it uses NYSE trading calendar logic to keep only the latest market day’s files ￼ – this is confirmed in code.
	•	Note that the risk toggle fix and Universe Builder v2 are still pending tasks, to set correct expectations.
	•	Mention the new “squeeze_watch” tag from short interest and the opening range breakout signals (break_above_range, etc.), as these are outcomes of the new data sources. These were likely implied before, but now we have specifics: e.g., the system flags stocks breaking above their 9:30-9:40 range ￼ or meeting a short-squeeze criteria.

By incorporating the above, the audit documentation will accurately reflect the current state of the repository. Overall, the audit summary was on point, and with the minor updates noted, it remains an accurate and complete description of the system. All core functionalities and their implementation align with what was described ￼ ￼, and the suggestions for improvement remain pertinent as future enhancements ￼.
